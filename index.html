<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Jiaye Leng</title>

    <meta name="author" content="Jiaye Leng">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/cityu.jpeg" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Jiaye Leng (冷佳业)
                </p>
                <p>I'm a PhD student at the City University of Hong Kong, working with <a href="https://sweb.cityu.edu.hk/hongbofu/index.htm">Prof. Hongbo Fu</a>. Currently my research focuses on Computer Graphics and Human-AI Interaction, and I am also interested in VR/AR technology.
                </p>
                <p>
                  I graduated with a Master's degree in Computer Technology from Beihang University, and a Bachelor's degree in Electronic Information Science and Technology from China University of Mining and Technology.
                </p>
                <p style="text-align:center">
                  <a href="mailto:jiayeleng2-c@my.cityu.edu.hk">Email</a> &nbsp;/&nbsp;
                  <a href="data/LJY_CV.pdf">CV</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=Ws3uaJEAAAAJ&hl=zh-CN&oi=ao">Scholar</a>
                </p>
              </td>
              <td style="padding:1.5%;width:15%;max-width:15%">
                <a href="images/ljy.JPG"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/ljy.JPG" class="hoverZoomLink"></a>
              </td>
            </tr>

          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;margin-bottom:-20px;"><tbody>
            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Publications</h2>
                <hr>
                <!--
                <p>
                  I'm interested in computer vision, deep learning, generative AI, and image processing. Most of my research is about inferring the physical world (shape, motion, color, light, etc) from images, usually with radiance fields. Representative papers are <span class="highlight">highlighted</span>.
                </p>
                -->
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;margin-bottom:-10px;"><tbody>
            
    <tr>
      <td style="padding:20px;width:20%;vertical-align:middle">
        <div class="one">
          <img src='images/ProInterAR.jpg' width=100%>
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a>
          <span class="papertitle">ProInterAR: A Visual Programming Platform for Creating Immersive AR Interactions</span>
        </a>
        <br>
    <a href="hhttps://huiye19.github.io/">Hui Ye*</a>,
    <strong>Jiaye Leng* (joint first author)</strong>,
    <a href="https://pengfeixu.com/">Pengfei Xu</a>,
    <a href="https://www.dgp.toronto.edu/~karan/">Karan Singh</a>,
    <a href="https://sweb.cityu.edu.hk/hongbofu/index.htm">Hongbo Fu</a>
        <br>
        <em>In Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems</em>, 2024
        <br>
        <a href="paper/ProInterAR.pdf">[paper]</a>
        |
        <a href="https://www.youtube.com/watch?v=n7rIRIqmWqA">[video]</a>
      </td>
    </tr>

  </tbody></table>
  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;margin-bottom:-20px;"><tbody>
      <tr>
      <td style="padding:20px;width:100%;vertical-align:middle">
        <hr>
        <p>Before PhD</p>
      </td>
    </tr>
  </tbody></table>
  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    
    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <img src='images/Light-Occlusion.jpeg' width=100%>
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a>
          <span class="papertitle">Light-Occlusion Text Entry in Mixed Reality</span>
        </a>
        <br>
    <a>Aoxin Sun</a>,
    <a href="https://liliwang.net/publications/">Lili Wang</a>,
    <strong>Jiaye Leng</strong>,
    <a>Sio Kei Im</a>
        <br>
        <em>In International Journal of Human-Computer Interaction</em>, 2023
        <br>
        <a href="paper/Light-Occlusion Text Entry in Mixed Reality.pdf">[paper]</a>
        |
        <a href="https://www.bilibili.com/video/BV1QK4y1B7LV/?vd_source=a17f0282b4b9fcc7f3e95becc47e4790">[video]</a>
        <!--
        /
        <a href="https://arxiv.org/abs/2312.07541">arXiv</a>
        <p></p>
        <p>
        Distilling a Zip-NeRF into a tiled set of MERFs lets you fly through radiance fields on laptops and smartphones at 60 FPS.
        </p>
          -->
      </td>
    </tr>

    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <img src='images/ProObjAR.jpeg' width=100%>
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a>
          <span class="papertitle">ProObjAR: Prototyping Spatially-aware Interactions of Smart Objects with AR-HMD</span>
        </a>
        <br>
    <a href="hhttps://huiye19.github.io/">Hui Ye</a>,
    <strong>Jiaye Leng</strong>,
    <a>Chufeng Xiao</a>,
    <a href="https://liliwang.net/publications/">Lili Wang</a>,
    <a href="https://sweb.cityu.edu.hk/hongbofu/index.htm">Hongbo Fu</a>
        <br>
        <em>In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems</em>, 2023
        <br>
        <a href="paper/ProObjAR.pdf">[paper]</a>
        |
        <a href="https://www.youtube.com/watch?v=YnkOdbIkooc">[video]</a>
        <!--
        /
        <a href="https://arxiv.org/abs/2312.07541">arXiv</a>
        <p></p>
        <p>
        Distilling a Zip-NeRF into a tiled set of MERFs lets you fly through radiance fields on laptops and smartphones at 60 FPS.
        </p>
          -->
      </td>
    </tr>

    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <img src='images/Flower Text Entry.jpg' width=100%>
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a>
          <span class="papertitle">Efficient Flower Text Entry in Virtual Reality</span>
        </a>
        <br>
    <strong>Jiaye Leng</strong>,
    <a href="https://liliwang.net/publications/">Lili Wang</a>,
    <a>Xiaolong Liu</a>,
    <a>Xuehuai Shi</a>,
    <a href="http://miaowang.me/">Miao Wang</a>
        <br>
        <em>In IEEE Transactions on Visualization and Computer Graphics</em>, 2022
        <br>
        <a href="paper/Efficient_Flower_Text_Entry_in_Virtual_Reality.pdf">[paper]</a>
        |
        <a href="https://www.youtube.com/watch?v=HxamSluP4_w">[video]</a>
        <!--
        /
        <a href="https://arxiv.org/abs/2312.07541">arXiv</a>
        <p></p>
        <p>
        Distilling a Zip-NeRF into a tiled set of MERFs lets you fly through radiance fields on laptops and smartphones at 60 FPS.
        </p>
          -->
      </td>
    </tr>

    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
          <img src='images/Bidirectional.png' width=100%>
        </div>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a>
          <span class="papertitle">Bidirectional Shadow Rendering for Interactive Mixed 360° Videos</span>
        </a>
        <br>
    <a href="https://liliwang.net/publications/">Lili Wang</a>,
    <a>Hao Wang</a>,
    <a>Danqing Dai</a>,
    <strong>Jiaye Leng</strong>,
    <a href="https://gaplab.cuhk.edu.cn/pages/people">Xiaoguang Han</a>
        <br>
        <em>In 2021 IEEE Virtual Reality and 3D User Interfaces</em>, 2021
        <br>
        <a href="paper/Bidirectional_Shadow_Rendering_for_Interactive_Mixed_360_Videos.pdf">[paper]</a>
        |
        <a href="">[video]</a>
        <!--
        /
        <a href="https://arxiv.org/abs/2312.07541">arXiv</a>
        <p></p>
        <p>
        Distilling a Zip-NeRF into a tiled set of MERFs lets you fly through radiance fields on laptops and smartphones at 60 FPS.
        </p>
          -->
      </td>
    </tr>

    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr>
        <td style="padding:0px">
          <br>
          <p style="text-align:center;font-size:small;">
            Website template from <a href="https://github.com/jonbarron/website">Jon Barron</a>
          </p>
        </td>
      </tr>
    </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
